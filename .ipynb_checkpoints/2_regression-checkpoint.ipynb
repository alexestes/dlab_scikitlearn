{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Boston Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we will use scikit-learn's [Boston](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) dataset. Instead of predicting discrete categories as we would in classification, with this dataset we can attempt to predict price, a continuous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to follow along in other tutorials in the scikit-learn documentation, you will need to know the data structures used as inputs to the models. Let'see what's in the boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description will tell us more about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are working on predicting median value of a home from 506 observations, and 13 covariates including crime rate, lot size, industry/commercial proportion, presence of the Charles River, nitric oxide concentration, rooms per dwelling, units built before 1940, distance to employment centers, access to highways, tax rate, school proxy, black population, and status. To get the variable names we can ask for them in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boston.feature_names)\n",
    "print()\n",
    "print(type(boston.feature_names))\n",
    "print()\n",
    "print(len(boston.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the input is a numpy array of strings for the variable labels. To get the variable data, we ask the dictionary for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(boston.data)\n",
    "print()\n",
    "print(type(boston.data))\n",
    "print()\n",
    "print(len(boston.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a numpy array, inside of which there is a separate array for each observation (all 506 for each hous, *not* 13 for each variable). Each inner array *must* lineup with the order of the variables *and* all other arrays. **ORDER MATTERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target (price), or *y* is accessed in the dictionary as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boston.target)\n",
    "print()\n",
    "print(type(boston.target))\n",
    "print()\n",
    "print(len(boston.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target array is only one dimension, lined up in order with the with the observations in the data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the input data, we need to split it up for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 75% of the data as training data, and 25% of the data as testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print()\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, as soon as you have `X_train`, `X_test`, `y_train`, and `y_test`, everything else is just a matter of choosing parameters for whichever model you choose. But this should not be trivialized, selecting models and that model's parameters is *very* important. While we will not cover it here, you should always select the model and parameters best suited for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax in scikit-learn does not change for each model, only the parameters. Examples of various models are given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a basic OLS linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lin_reg = linear_model.LinearRegression(n_jobs=1)  # CPUs to use)\n",
    "\n",
    "model = lin_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ridge_reg = linear_model.Ridge(alpha=1.0,  # regularization\n",
    "                               normalize=True,  # normalize X regressors\n",
    "                               solver='auto',\n",
    "                               random_state = 10)  # options = ‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag'\n",
    "\n",
    "model = ridge_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elastic_reg = linear_model.ElasticNet(alpha=1.0,  # penalty, 0 is OLS \n",
    "                               random_state=10,\n",
    "                               selection='cyclic')  # or 'random', which converges faster\n",
    "\n",
    "model = elastic_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv_reg = svm.SVR(kernel='linear',  # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "                 degree=3,  # only used for 'poly' above\n",
    "                 gamma='auto',  # kernal coeff, default auto is 1/n_features\n",
    "                 C=1.0)\n",
    "\n",
    "model = sv_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn_reg = neighbors.KNeighborsRegressor(n_neighbors=5,\n",
    "                                        weights='uniform',  # ‘distance’ weights points by inverse of their distance\n",
    "                                        algorithm='auto',  # out of ‘ball_tree’, ‘kd_tree’, ‘brute’\n",
    "                                        leaf_size=30)  # for tree algorithms\n",
    "\n",
    "model = knn_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rf_reg = ensemble.RandomForestRegressor(n_estimators=10,  # number of trees\n",
    "                                        criterion='mse',  # how to measure fit\n",
    "                                        max_depth=None,  # how deep tree nodes can go\n",
    "                                        min_samples_split=2,  # samples needed to split node\n",
    "                                        min_samples_leaf=1,  # samples needed for a leaf\n",
    "                                        min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                                        max_features='auto',  # max feats\n",
    "                                        max_leaf_nodes=None,  # max nodes\n",
    "                                        n_jobs=1, # how many to run parallel\n",
    "                                        random_state=10)\n",
    "\n",
    "model = rf_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting - AdaBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ab_reg = ensemble.AdaBoostRegressor(base_estimator=None,  # default is DT \n",
    "                                    n_estimators=50,  # number to try before stopping\n",
    "                                    learning_rate=1.0,  # decrease influence of each additional estimator\n",
    "                                    random_state=10,\n",
    "                                    loss='linear')  # also ‘square’, ‘exponential’\n",
    "\n",
    "\n",
    "model = ab_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Regression (*new in scikit-learn .18*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neural_network\n",
    "nn_reg = neural_network.MLPRegressor(hidden_layer_sizes=(7, ),  # neurons in ith layer\n",
    "                                     activation='relu',  # activation function, also 'identity', 'logistic', 'tanh'\n",
    "                                     solver='adam',  # solves weight optimization, also ‘lbfgs’, ‘sgd’, ‘adam’\n",
    "                                     alpha=0.0001,  # L2 penalty\n",
    "                                     batch_size='auto',  # auto is 200\n",
    "                                     learning_rate='constant',  # schedule for weight updates, also ‘invscaling’, ‘adaptive’\n",
    "                                     learning_rate_init=0.001,  # lr initial value for sgd and adam\n",
    "                                     max_iter=200,  # max iterations\n",
    "                                     momentum=0.9,  # for gradient descent, only sgd\n",
    "                                     early_stopping=False,  # if validation not improving\n",
    "                                     validation_fraction=0.1,  # validation subset\n",
    "                                     random_state=10,\n",
    "                                     beta_1=0.9,  # decay for adam first momentum\n",
    "                                     beta_2=0.999)  # decay for adam second momentum\n",
    "\n",
    "model = nn_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our division of training and testing data was naïve, ideally you will do k-fold cross-validation, as seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(sv_reg, boston.data, boston.target, cv=3)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, scikit-learn uses 3-fold cross-validation, but 5- and 10-fold are also common.\n",
    "\n",
    "Generally you will then choose the model which has the best cross-validation score, and then test it on the held out test data. After that you do not make any further changes to the model, in order to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Write a function that computes the 5-fold CV score for all of our models. Use it to determine which model we should apply to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
