{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsuperivsed ML method used to group data points based on their features alone, and no observed grouping labels as in supervised classification. Thus most clustering alorithms seeks to group points by their distance in a high dimensional space generated by provided features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) K-means clustering  \n",
    "\n",
    "In this section we will cover k-means clustering using `scikit-learn`. The scikit-learn documentation for clustering is found [here](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "First we'll import `KMeans` and `numpy` so that we can make our arrays. The `%matplotlib inline` will make our plots show up within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a few points. Remember, as with classification and regression, our data should be in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,1], [1,2], [1, 0], [-1, -3],\n",
    "             [15, 21], [18, 30], [20, 20], [22, 19],\n",
    "             [45, 50], [42, 48], [60, 40], [50, 50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot them we can see that they appear to be arranged roughly in three groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(*X.T)\n",
    "plt.ylabel('random points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our clusters, all we have to do is specify how many we want, and then fit the model to the data. We'll choose 3. We can also specify the maximum number of iterations of the k-means algorithm, which you may want to do with a much larger dataset.\n",
    "\n",
    "First thing's first: **set a random seed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3,\n",
    "               max_iter=300 #default\n",
    "               ).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the centers of the clusters through the `cluster_centers_` attribute. To get the labels (i.e. the corresponding cluster) we use `labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Centers\")\n",
    "print(kmeans.cluster_centers_)\n",
    "print()\n",
    "\n",
    "print(\"Labels\")\n",
    "print(kmeans.labels_)\n",
    "print()\n",
    "\n",
    "for point, label in zip(X, kmeans.labels_):\n",
    "    print(\"Coordinates:\", point, \"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also plot out cluster centers along with the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*X.T, s=50, c='b', label='original points')\n",
    "ax1.scatter(*kmeans.cluster_centers_.T, s=50, c='r', label='cluster centers')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see to which cluster a new point would belong, we simply use the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_points = np.asarray([[0, 4],\n",
    "                        [19, 25],\n",
    "                        [40, 50]])\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print()\n",
    "\n",
    "print(\"0, 4\")\n",
    "print(\"Cluster:\", kmeans.predict([[0, 4]]))\n",
    "print()\n",
    "\n",
    "print(\"19, 25\")\n",
    "print(\"Cluster:\", kmeans.predict([[19, 25]]))\n",
    "print()\n",
    "\n",
    "print(\"40, 50\")\n",
    "print(\"Cluster:\", kmeans.predict([[40, 50]]))\n",
    "\n",
    "#plot new points\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*X.T, s=50, c='b', label='original points')\n",
    "ax1.scatter(*kmeans.cluster_centers_.T, s=50, c='r', label='cluster centers')\n",
    "ax1.scatter(*new_points.T, s=50, c='g', label='new points')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll show an example of agglomerative clustering, which is a type of hierarchical clustering. The documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) in case you want to know more about the parameters. We'll use some of scikitlearn's toy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 1500\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)[0]\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=0)[0]\n",
    "\n",
    "plt.scatter(*noisy_moons.T)\n",
    "plt.ylabel('noisy moons')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(*blobs.T)\n",
    "plt.ylabel('blobs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use two clusters this time, and use ward linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ward = AgglomerativeClustering(n_clusters=2,\n",
    "                               linkage='ward', #linkage can be ward (default), complete, or average\n",
    "                               affinity='euclidean') #affinity must be euclidean if linkage=ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit the clustering model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ward.fit(noisy_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll sort the points by label and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 0])\n",
    "one = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 1])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same with the blobs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ward.fit(blobs)\n",
    "\n",
    "ward.labels_\n",
    "\n",
    "zero = np.array([point for label, point in zip(ward.labels_, blobs) if label == 0])\n",
    "one = np.array([point for label, point in zip(ward.labels_, blobs) if label == 1])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: DBSCAN \n",
    "\n",
    "\n",
    "It looks like our agglomerative clustering model did not cluster the noisy moons dataset how we might have wanted. For the challenge, use [`DBSCAN`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) to cluster noisy moons. Then plot the results and see what it looks like. Try an `eps` value of .2. This sets the maximum distance between two samples for them to be considered in the same neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
